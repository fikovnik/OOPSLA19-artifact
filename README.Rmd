---
title: "OOPSLA19 Artifact - Scala Implicits are Everywhere"
authors: "Filip Krikava, Jan Vitek and Heather Miller"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

## Artifact Description

This is the artifact for the paper _Scala Implicits are Everywhere_ at OOPSLA 2019.
The aim is to:

1. show extracting implicit declarations and call sites from Scala code,
1. demonstrate running the analysis pipeline, and
1. make available the large corpus of Scala projects used for the paper.

The artifact is composed of 3 parts.
First part uses an example Scala project to show an implicit extraction and an overview of the underlying model.
Second part demonstrates the analysis on a set of 50 randomly selected Scala projects.
Finally, the third part will look at the results of the analysis pipeline on the corpus used for paper.

## Requirements

The pipeline depends on a number of tools.
The full requirements are listed in the documentation of the [analysis pipeline itself](https://github.com/PRL-PRG/scala-implicits-analysis/tree/oopsla19#requirements)
To make it more convenient we have build a docker image that has all these dependencies installed.
The image is available on [Docker HUB](https://hub.docker.com/r/prlprg/oopsla19-scala).
This image can be either pulled directly from the Hub or <a href="#building-image-locally">build locally</a>.
To run the image, you will need [docker community edition](https://docs.docker.com/install/) version 18+ and bash.

The docker image only includes the dependencies needed to build the pipeline.
This allows one to start from scratch without the need of installing any tools.
However, building the pipeline and compiling the Scala projects that are part of the corpora included in this artifact requires a large number of external dependencies.
We have packaged these in an tarball so one can start with a hot cache.
Similarly, building Scala projects is a resource intensive project.
We could use only small projects, but they will not be very representative of the real world.
Instead we have also packaged the built corpus which can be used to speed up the evaluation (in the end, the intention is not to test SBT or Scala compiler).
However, it is perfectly fine to start from scratch skipping the optional steps bellow.
One only need a bit more patience.

This artifact requires about ~13GB of free space.

## Getting Started Guide

The following steps are intended for the _kick-the-tires phase_ to ensure that all the components work.
If you run into some problems, please check the <a href="#troubleshooting">troubleshooting</a> section we have added at the end.
This guide was tested on Linux and OSX.

1. Clone the artifact

   ```{sh eval=FALSE}
   git clone https://github.com/fikovnik/OOPSLA19-artifact
   ```

   Unless stated otherwise, all the subsequent commands should be run _inside the artifact's directory_:

   ```{sh eval=FALSE}
   cd OOPSLA19-artifact
   ```

1. Clone the analysis pipeline:

   It is a separate open source project hosted on GitHub.

   ```{sh eval=FALSE}
   git clone --single-branch --branch oopsla19 https://github.com/PRL-PRG/scala-implicits-analysis
   ```

   It is _important_ that you use =oopsla19= branch:

   ```{sh eval=FALSE}
   git --git-dir=scala-implicits-analysis/.git branch
   * oopsla19
   ```

   We used the `3004cf4ff55df52ae34a3c33bcc6100fafc4e9a4` commit for creating this guide.

   After cloning, the artifacts directory should look like:

   ```
   ├── corpora
   │   ├── 1-example            -- an example project to demonstrate implciti extraction
   │   ├── 2-single             -- a single real-world Scala project to check the pipeline
   │   ├── 3-sample-set         -- a sample set of random Scala projects
   │   └── 4-github             -- placeholder for the entire analyzed repository (not downloaded yet)
   ├── docker                   -- the source code for building the docker image
   │   ├── Dockerfile
   │   ├── entrypoint.sh
   │   └── Makefile
   ├── scala-implicits-analysis -- the analysis pipeline (clonned in the last step)
   │   └── ...
   ├── README.Rmd               -- these instructions
   └── run.sh                   -- this guide
   ```

1. Download the cache files (_optional_)

   This includes cache for SBT, [Ivy](ant.apache.org/ivy) (the dependency manager used by SBT) and [coursier](https://github.com/coursier/coursier) (alternative artifact fetcher used by some Scala projects).
   Even with this cache, SBT might still decide to download some dependencies, but populating the cache will speed things up.
   The download size is 2.6GB.

   ```{sh eval=FALSE}
   curl -o cache.tar.xz https://owncloud.cesnet.cz/index.php/s/hUbBGxG2cgih4h9/download
   ```

   The MD5SUM is `b25a4533a3c833e689827a15a3600918`.

   ```{sh eval=FALSE}
   tar xfvJ cache.tar.xz
   ```

   After downloading and extracting it should look like:

   ```
   ├── cache
   │   ├── coursier            -- coursier cache
   │   ├── ivy                 -- ivy cache
   │   └── sbt                 -- sbt cache
   ├── corpora
   │   └── ...
   ├── docker
   │   └── ...
   ├── scala-implicits-analysis
   │   └── ...
   └── ...
   ```

1. Download the built corpora (_optional_)

   This includes cloned Scala projects that are part of the corpora (one project for the `2-single` and 50 projects for the `3-sample-set` corpora).
   The projects are already built.
   Please not that the rest of the pipeline still needs to be run, it is only the project building that will be skipped.
   The download size is 500MB.

   ```{sh eval=FALSE}
   curl -o corpora.tar.xz https://owncloud.cesnet.cz/index.php/s/tI7FIxneRjSjSMW/download
   ```

   The MD5SUM is `729bc166e24c08ef116d4c9bee713c73`.

   ```{sh eval=FALSE}
   tar xfvJ corpora.tar.xz
   ```

   After downloading and extracting it should look like:

   ```
   ├── cache
   │   └── ...
   ├── corpora
   │   ├── 1-example
   │   │   └── ...
   │   ├── 2-single
   │   │   ├── all-projects   -- includes the built projects
   │   │   └── ...
   │   ├── 3-sample-set
   │   │   ├── all-projects   -- includes the built projects
   │   │   └── ...
   │   └── 4-github
   │       └── ...
   ├── docker
   │   └── ...
   ├── scala-implicits-analysis
   │   └── ...
   └── ...
   ```

1. Make sure you have a docker installed and running

   ```{sh eval=FALSE}
   docker run --rm hello-world
   ```

1. Pull the docker image (or <a href="#building-image-locally">build one
   locally</a>)

   ```{sh eval=FALSE}
   docker pull prlprg/oopsla19-scala
   ```

1. Check that the image can be run

   The image needs a number of directories and environment variables set which would make it cumbersome to provide each time.
   There is a bash script `run.sh` that encapsulates all the necessary settings.
   Please use this script to run the image.

   ```{sh eval=FALSE}
   ./run.sh id
   uid=1000(scala) gid=1000(scala) groups=1000(scala),27(sudo)
   ```

   The `uid` and `gid` returned should be the same as are on your host system.
   You can double check that by running `id -u` and `id -g`.
   This is important because while the commands will run in the docker image, they will modify the files in this directory.
   If the `uid` or `gid` are different it will be inconvenient for you to view the results and remove them at the end.

   You might see a warning like:

   ```
   groupmod: GID '20' already exists
   ```

   This can be ignored (most likely you are on OSX which assigns lower GID to the user group).

1. Build the pipeline

   The pipeline consists of the following tools:
   - sbt-plugins - a SBT plugin adding `metadata` and `semanticdb` commands for metadata extraction and semanticdb generation.
   - libs - the model of implicits, the implicit extractor and a number of tools that export the results into CSV files.
   - scripts/tools - a modified version of [Dejavu](https://github.com/PRL-PRG/dejavu-artifact) allowing to index Scala files.

   To build these tools, run the following:

   ```{sh eval=FALSE}
   ./run.sh make -C scala-implicits-analysis
   ```

1. Run the pipeline on a single project corpus

   We will run the pipeline on a corpus that contains only one, yet real Scala project.
   The corpus is located in `corpora/2-single` directory.

   ```{sh eval=FALSE}
   ./run.sh make -C corpora/2-single
   ```

1. Check the results

   After the make is done, there should be three HTML files in the corpus directory:

   ```{sh eval=FALSE}
   ls -1 corpora/2-single/*.html
   implicits-analysis.html
   stage1-analysis.html
   stage3-analysis.html
   ```

   The `stage*-analysis.html` describes the state of the corpus while the `implicits-analysis.html` summarizes the extracted implicits. The details of what to expect in each of these files follow in the part 2 of this artifact.
   Please mind that the reports were prepared for large corpus and thus with only a one project some of the tables will be either empty or useless.
   The same applies to the plots which might not be scaled properly.

This concludes the getting started guide.

## Part 1

In this part we will show the implicit extractor on a simple Scala project that is in `corpora/1-example`.

### Overview of the project

The project has two modules: `typeclass` and `conversion`.
The `typeclass` module has just source file:

- `corpora/1-example/typeclass/src/main/scala/example/TypeClass.scala`: defines a type class `Jsonable[T]` together with two instances: `intJsonable` (for integers) and `seqJsonable` (for sequences of `Jsonable[T])` and a function `asJson` which for a given type `T`, for which there exists an instance of `Jsonable[T]` type class, produces a JSON string:

  ```scala
  // type class
  trait Jsonable[T] {
    def toJson(x: T): String
  }

  // an instance of the type class for `Int`
  implicit val intJsonable = new Jsonable[Int] {
    def toJson(x: Int): String = x.toString
  }

  // an implicit type class derivation method which based on an instance of `Jsonable[T]`
  // derives a type class that works for `Jsonable[Seq[T]]`
  implicit def seqJsonable[T: Jsonable] = new Jsonable[Seq[T]] {
    def toJson(xs: Seq[T]): String = {
      xs.map(asJson(_)).mkString("[", ",", "]")
    }
  }

  // this is a function using the `Jsonable` type class
  def asJson[T](x: T)(implicit tc: Jsonable[T]): String = {
    tc.toJson(x)
  }
  ```

The `conversion` module has three files:

- `corpora/1-example/conversion/src/main/scala/example/Conversion.scala`: defines an extension method that for any given type `T` for which there exists an instance of `Jsonable[T]` type class, defines a `toJson` method:

  ```scala
  implicit class XtensionJson[T: Jsonable](x: T) {
    def toJson: String = implicitly[Jsonable[T]].toJson(x)
  }
  ```

- `corpora/1-example/conversion/src/test/scala/example/ConversionTest.scala`: a test for the extension method using scala-test:

  ```scala
  test("JSON conversion should convert a sequence of integers") {
    Seq(1,2,3).toJson shouldBe "[1,2,3]"
  }
  ```

- `corpora/1-example/conversion/src/main/scala/example/Cards.scala`: an example for Figure 1 from the paper:

  ```scala
  // When run, it should throw java.lang.IndexOutOfBoundsException: 1
  object Cards extends App {
    case class Card(n: Int, suit: String = "club") {
      def isInDeck(implicit deck: List[Card]) = deck contains this
    }

    implicit val dk = List(Card(1))
    implicit def iToC(n: Int) = Card(n)

    1.isInDeck
  }
  ```

### Extracting implicits

In order to run the implicit extractor, we need to extract the project metadata and generate semanticdb.
We have built an SBT plugin that does that.
Running:

```{sh eval=FALSE}
./run.sh make -C corpora/1-example
```

should do all these tasks including export of the extracted implicits from the binary model into CSV files.
After the command has run, the following files (among others) should be created:

In `_analysis_`:

- `metadata-modules.csv`      -- information about project modules
  - it shows three modules (there is an aggregating root module) and all the version information
- `metadata-dependencies.csv` -- information about project dependencies
  - for example the `conversion` module has 11 dependecies out of which 9 are in the test scope and 7 are transitive
- `metadata-sourcepaths.csv`  -- information about projects' source paths
  - there are three entries since only `conversion` defines some tests
  - there are also number of lines of Scala code
- `semanticdb.bin`            -- merged semanticdb files
- `implicits.bin`             -- extracted implicits in google protocol buffer format following our [model](https://github.com/PRL-PRG/scala-implicits-analysis/blob/master/libs/model/src/main/protobuf/model.proto)
- `implicits-stats.csv`       -- stats about how many implicits were extracted
- ``implicits-exceptions.csv` -- problems encountered when extracting implicits

In root:
- `implicit-callsites.csv`    -- extracted implicit call sites
- `implicit-declarations.csv` -- extracted implicit declarations
- `implicit-conversions.csv`  -- extracted implicit conversions
- `implicit-parameters.csv`   -- extracted implicit parameters

### Checking the results

Following are the some of the results to be checked.

1. All the `*-problems.csv` files should be empty as weel as `_analysis_/implicits-exceptions.csv`:

   ```{sh eval=FALSE}
   wc -l corpora/1-example/*-problems.csv corpora/1-example/_analysis_/implicits-exceptions.csv
   1 implicit-callsites-problems.csv
   1 implicit-conversions-problems.csv
   1 implicit-declarations-problems.csv
   1 implicit-parameters-problems.csv
   1 _analysis_/implicits-exceptions.csv
   ```
   Note:
   - they should contain 1 line - the CSV header

1. There should be 7 local implicit declarations:
   - 3 for `TypeClass.scala`,
   - 1 for `Conversion.scala`, and
   - 3 for `Cards.scala`

   The easiest way to check this is to see how many `implicit_local_declarations` are in `_analysis_/implicits-stats.csv`.
   Details about each declaration is in `implicit-declarations.csv`, but also include all the implicit declarations from dependencies. One way how to quickly filter is using:

   ```{sh eval=FALSE}
   cat corpora/1-example/implicit-declarations.csv | grep TypeClass.class
   ```

   to see declarations defined in the `TypeClass.scala`.

1. There should be 4 implicit conversions

   ```{sh eval=FALSE}
   ./run.sh Rscript -e 'glue::glue_data(readr::read_csv("corpora/1-example/implicit-conversions.csv"), "- `{declaration_id}`: `{from}` => `{to}`")'
   ```

   - `org/scalatest/Matchers#convertToStringShouldWrapper().: java/lang/String# => org/scalatest/Matchers#StringShouldWrapper#`
   - `example/Conversion.XtensionJson().: example/Conversion.XtensionJson().[T] => example/Conversion.XtensionJson#[example/Conversion.XtensionJson().[T]]`
   - `example/Cards.iToC().: scala/Int# => example/Cards.Card#`
   - `example/Cards.dk.: scala/Int# => example/Cards.Card#`

   This correctly identify the `implcit val dk = List(...)` as implicit conversion as indeed `List[T]` is of a functional type `Function1[Int, T]`.

1. There should be 5 implicit parameters:

   ```{sh eval=FALSE}
   ./run.sh Rscript -e 'glue::glue_data(readr::read_csv("corpora/1-example/implicit-parameters.csv"), "- `{declaration_id}`")' | grep example
   ```

   - `example/TypeClass.seqJsonable().`
   - `example/TypeClass.asJson().`
   - `example/TypeClass.seqJsonable().`
   - `example/Conversion.XtensionJson().`
   - `example/Cards.Card#isInDeck().`

### Playing with the model

The above CSV files are what we use for the subsequent analysis which is done in R.
However, the model stored in the `implicits.bin` contains much more information.
It is possible to access it and query in a Scala REPL using ammonite:

```{sh eval=FALSE}
./run.sh -C corpora/1-example console
```

This starts a Scala REPL and provide a function that can load the `implcits.bin` file:

```scala
@ val project = loadImplicits("_analysis_/implicits.bin")
```

For example, to get the declarations of the implcit call sites:

```scala
@ project.implicitCallSites.map(_.declarationId)
res2: Iterable[String] = List(
  "example/TypeClass.seqJsonable().",
  "example/TypeClass.asJson().",
  "example/TypeClass.asJson().",
  "scala/collection/Seq.canBuildFrom().",
  "scala/collection/TraversableLike#map().",
  "scala/Predef.implicitly().",
  "example/Cards.Card#isInDeck().",
  "example/Conversion.XtensionJson().",
  "example/TypeClass.seqJsonable().",
  "org/scalactic/source/Position.apply().",
...
```

To get more ideas what to query, please consult the [model](https://github.com/PRL-PRG/scala-implicits-analysis/blob/master/libs/model/src/main/protobuf/model.proto).

## Part 2

The second part of the artifact will demonstrate the analysis pipeline on a sample set of Scala projects.
This set has been created randomly from the entire corpus we have analyzed for the paper using the following [script](https://github.com/PRL-PRG/scala-implicits-analysis/blob/oopsla19/scripts/analysis/random-github-projects.Rmd).

### Overview of the pipeline

 - Stage 1:

    1. create `all-projects.txt` that contains a new-line separated list of project_id (`<github-user-name>--<github-repo-name>`) (`status=0`),
    2. download projects into `all-projects` folder and optionally patch them with information in `all-projects-patches.csv` (`status=1`),
    3. extract `repo-metadata.csv` that contains basic metadata about the repository including scala lines of code, build system and optionally SBT version,
    4. filter non-empty and SBT compatible projects (projects using SBT >= 0.13.5 or 1.0) (`status=2`),
    5. fetch information from GitHub about all the SBT projects
    6. run dejavu
    7. create `corpus-stage1.csv` summary
    8. run the `stage1-analysis.Rmd` to produce the `projects.txt` file

- Stage 2:

    9. compile all projects
    10. extract SBT metadata for each project
    11. extract semanticdb for each project

- Stage3:

    10. extract implicits for each project
    10. merge all the results
    11. export implicits into CSV files
    12. create `corpus-stage3.csv` summary
    13. run the `stage3-analysis.Rmd`

All these tasks are run from a `Makefile.corpus` makefile.

### Running the pipeline

```{sh eval=FALSE}
./run.sh make -C corpora/3-sample-set
```

### Results

Once the analysis is complete it should generate three HTML files:

- `stage1-analysis.html`
- `stage3-analysis.html`
- `implicits-analysis.html`

They are the results of running the following Rmd notebooks:

- [stage1-analysis.Rmd](https://github.com/PRL-PRG/scala-implicits-analysis/blob/oopsla19/scripts/analysis/stage1-analysis.Rmd)
- [stage3-analysis.Rmd](https://github.com/PRL-PRG/scala-implicits-analysis/blob/oopsla19/scripts/analysis/stage3-analysis.Rmd)
- [implicits-analysis.Rmd](https://github.com/PRL-PRG/scala-implicits-analysis/blob/oopsla19/scripts/analysis/implicits-analysis.Rmd)

The first two provide an overview of the corpus and were used of Section 4 of the paper as well as for Figure 4.
The last one summarizes the extracted implicits which was used to compose Section 5 and Figures 5 and 6.


### Custom corpus

You are welcome to try it on your favorite Scala projects.
To create a new corpus, simply create a new directory in `corpra` and do the following:

```{sh eval=FALSE}
mkdir corpora/5-my-corpus
cd corpora/5-my-corpus
ln -s ../../scala-implicits-analysis/Makefile.corpus Makefile
ln -s ../../scala-implicits-analysis/all-projects-patch.csv .
cp ../2-single/{projects-github-info.csv.pinned,scaladex.txt.pinned} .
```

Then create `all-projects.txt` file and put there the names of projects in the `<gh-owner>--<repo-name>` format separated by new lines.

### A note about concurrency

The pipeline is meant to be run in parallel.
The `N_JOBS` variable controls the number of parallel jobs spawned by GNU parallel.
While the jobs are running, the level of parallelism can be controlled by `jobsfile.txt` in the root of each corpus (e.g. `corpora/3-sample-set/jobsfile.txt`).
However, please keep in mind that compiling Scala code is resource intensive.
On as 72 core Intel Xeon 6140 @ 2.60GHz with 256GB RAM we managed to get ~12 projects compiling in parallel before saturation.

## Part 3

The analysis presented in the paper was done on all Scala projects we were able to download from GitHub.
The corpus was bootstrapped using [GHTorrent](http://ghtorrent-downloads.ewi.tudelft.nl/mysql/mysql-2019-01-01.tar.gz) and [scaladex](https://index.scala-lang.org/).
From these two sources, we extracted projects names (GitHub repository owner and repository name) and downloaded them to our server.
The corpus of all the GitHub Scala projects is available on server at:

```
http://prl1.ele.fit.cvut.cz:8149/github
```

If you are concerned about privacy you can access it via a VPN service or TOR although we do not keep any access logs.

The reason why do not create an archive of it is its size:

1. The cache similar to what we have used in this artifact has 110GB:
   - 29GB `cache/coursier`
   - 79GB `cache/ivy`
   -  2GB `cache/sbt/sbt-boot`

2. The corpus itself has 1.1TB.

It is possible to replace the results just by downloading the final feather files.
The download size is about 4.5GB.

```{sh eval=FALSE}
./run.sh make -C corpora/4-github fetch
```

and then running the same reports:

```{sh eval=FALSE}
./run.sh make -C corpora/4-github report
```

Please keep in mind that you will need plenty of RAM to load all the data into memory for R to process them.
It is possible to fit it into 16GB RAM if most of it is free.
If this is not feasible you can check the generated files directly:
- [stage1-analysis.html](http://prl1.ele.fit.cvut.cz:8149/github/stage1-analysis.html)
- [stage3-analysis.html](http://prl1.ele.fit.cvut.cz:8149/github/stage3-analysis.html)
- [implicits-analysis.html](http://prl1.ele.fit.cvut.cz:8149/github/implicits-analysis.html)

## Building image locally
<a name="#building-image-locally"/>

To build image locally you can either use make by running:

```{sh eval=FALSE}
make -C docker
```

or by directly run `docker build`:

```{sh eval=FALSE}
docker build -t prlprg/oopsla19-scala docker
```

## Toubleshooting
<a name="troubleshooting"/>

### Docker on OSX

It is better to use homebrew cask to install docker:

```sh
brew cask install docker
```

in case you see `docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?.` error message
cf: https://stackoverflow.com/a/44719239

### Docker on Linux

In some distribution the package does not add the current user to `docker` group.
In this case, either add yourself to `docker` group or run the `./run*` commands with `sudo`.

### Error: `gpg: keyserver receive failed: Cannot assign requested address= when building image` while building the docker image

The keyserver that is used to add debian APT repositories for newer version of R and SBT sometimes does not work.
What has worked all the time was simply repeating the image build process by rerunning the make.


### Error: A compilation error `Caused by: scala.reflect.internal.MissingRequirementError: object scala in compiler mirror not found.`

Rebuild [BuildInfo](scala-implicits-analysis/libs/tools/target/scala-2.12/src_managed/main/sbt-buildinfo/BuildInfo.scala) file by deleting the file and running `./run.sh make -C scala-implicits-analysis/libs`.

### Out of memory exception

Compiling Scala projects can be rather expensive, especially for larger projects.
In `scala-implicits-analysis/Makevars` you can adjust:

- `SBT_MEM ?= 8192` the `-Xmx` memory for running SBT
- `MAX_MEM ?= 8192` the `-Xmx` memory for running ammonite (the `.sc` scripts)
- `N_JOBS ?= 1` the number of projects processed in parallel

The default is 8GB.

### Timeout

In case the compilation, metadata extraction, semanticdb generation or implicit extraction timeouts, you can try increasing the timeout for parallel taks by setting the `TIMEOUT` environment variable inside the docker container or in the `scala-implicits-analysis/Makevars`.
The default is 30 minutes.
